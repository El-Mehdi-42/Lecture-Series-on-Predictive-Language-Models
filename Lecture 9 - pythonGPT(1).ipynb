{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRX5jVvVrvSs"
   },
   "source": [
    "# Importing packages and installing openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N9ShHxhbb-IU"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kt3b4Zz1V5-M",
    "outputId": "6c7168a9-3bcc-445d-9eb5-f83086efb10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in /Users/issouani/anaconda3/lib/python3.11/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/issouani/anaconda3/lib/python3.11/site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/issouani/anaconda3/lib/python3.11/site-packages (from openai==0.28) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/issouani/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-IJb4War0k0"
   },
   "source": [
    "### You should change ```\"your-key\"``` with your key from <a>https://platform.openai.com/api-keys</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-1en_WicWJ3f"
   },
   "outputs": [],
   "source": [
    "#key = \"your-key\"\n",
    "import openai\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxdff8lMpxT5"
   },
   "source": [
    "### First test\n",
    "To check if it works and see what information we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlKUoDlGn-xH",
    "outputId": "5f98d989-db66-438c-87db-5a22823f62f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "de la nourriture, comme des croquettes, de la pâtée ou des restes de viande\n",
      "\n",
      "['index', 'text', 'logprobs', 'finish_reason']\n",
      "['tokens', 'token_logprobs', 'top_logprobs', 'text_offset']\n",
      "['\\n\\n', 'de', ' la', ' nour', 'rit', 'ure', ',', ' comme', ' des', ' cro', 'quet', 'tes', ',', ' de', ' la', ' p', 'ât', 'ée', ' ou', ' des', ' rest', 'es', ' de', ' vi', 'ande']\n",
      "[0.256, 0.411, 0.998, 0.946, 1.0, 1.0, 0.388, 0.363, 0.915, 1.0, 1.0, 1.0, 0.758, 0.885, 1.0, 0.985, 1.0, 0.937, 0.886, 0.404, 0.459, 1.0, 0.979, 0.539, 0.999]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Effectuer une requête avec l'API de complétion pour obtenir les logprobs\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",  # Ou \"gpt-4\" si vous y avez accès\n",
    "    prompt=\"Le chat mange...\",  # Votre texte d'entrée\n",
    "    max_tokens=25,  # Nombre maximal de tokens pour la réponse\n",
    "    logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "    top_p=1,\n",
    "    presence_penalty=0.5,\n",
    "    frequency_penalty=0.5,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(response['choices'][0][\"text\"], end=\"\\n\\n\")\n",
    "print([i for i in response['choices'][0]])\n",
    "print([i for i in response['choices'][0][\"logprobs\"]])\n",
    "print(response['choices'][0][\"logprobs\"][\"tokens\"])\n",
    "print([round(np.exp(i),3) for i in response['choices'][0][\"logprobs\"][\"token_logprobs\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6usL_t-qLu_"
   },
   "source": [
    "### What we can get ?\n",
    "So, we can get :\n",
    "1. The text > The actual output\n",
    "2. the logprobs\n",
    "  1. The tokens (of the output)\n",
    "  2. their corresponding logprobs\n",
    "  3. top other tokens with their respective logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn1R541Ar-YQ",
    "outputId": "4538d641-c619-4fbb-a22c-d247c0edb1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestion n°0\t,\t('\\n\\n', 0.256),\t('\\n', 0.232),\t(' des', 0.035),\t(' du', 0.019),\t(' un', 0.018)\n",
      "\n",
      "Suggestion n°1\t,\t('de', 0.411),\t('des', 0.141),\t('du', 0.088),\t('La', 0.059),\t('...', 0.048)\n",
      "\n",
      "Suggestion n°2\t,\t(' la', 0.998),\t(' tout', 0.001),\t(' l', 0.0),\t(' nombreux', 0.0),\t(' cro', 0.0)\n",
      "\n",
      "Suggestion n°3\t,\t(' nour', 0.946),\t(' vi', 0.045),\t(' sour', 0.004),\t(' p', 0.003),\t(' cro', 0.001)\n",
      "\n",
      "Suggestion n°4\t,\t('rit', 1.0),\t('r', 0.0),\t('ri', 0.0),\t('iture', 0.0),\t('<|endoftext|>', 0.0)\n",
      "\n",
      "Suggestion n°5\t,\t('ure', 1.0),\t('ur', 0.0),\t('u', 0.0),\t('<|endoftext|>', 0.0),\t('ue', 0.0)\n",
      "\n",
      "Suggestion n°6\t,\t(',', 0.388),\t('.', 0.31),\t(' ou', 0.111),\t(' comme', 0.062),\t(' pour', 0.037)\n",
      "\n",
      "Suggestion n°7\t,\t(' comme', 0.363),\t(' des', 0.205),\t(' princip', 0.186),\t(' géné', 0.119),\t(' de', 0.047)\n",
      "\n",
      "Suggestion n°8\t,\t(' des', 0.915),\t(' du', 0.055),\t(' de', 0.026),\t(' par', 0.001),\t(' les', 0.001)\n",
      "\n",
      "Suggestion n°9\t,\t(' cro', 1.0),\t(' biscuits', 0.0),\t(' al', 0.0),\t(' sour', 0.0),\t(' gran', 0.0)\n",
      "\n",
      "Suggestion n°10\t,\t('quet', 1.0),\t('quette', 0.0),\t('q', 0.0),\t('<|endoftext|>', 0.0),\t('qu', 0.0)\n",
      "\n",
      "Suggestion n°11\t,\t('tes', 1.0),\t('t', 0.0),\t('te', 0.0),\t('<|endoftext|>', 0.0),\t('es', 0.0)\n",
      "\n",
      "Suggestion n°12\t,\t(',', 0.758),\t(' ou', 0.238),\t(' pour', 0.002),\t(' spéc', 0.001),\t(' de', 0.0)\n",
      "\n",
      "Suggestion n°13\t,\t(' de', 0.885),\t(' du', 0.074),\t(' des', 0.04),\t(' p', 0.0),\t(' ou', 0.0)\n",
      "\n",
      "Suggestion n°14\t,\t(' la', 1.0),\t(' l', 0.0),\t(' p', 0.0),\t(' ', 0.0),\t(' pat', 0.0)\n",
      "\n",
      "Suggestion n°15\t,\t(' p', 0.985),\t(' vi', 0.013),\t(' pat', 0.002),\t(' nour', 0.0),\t(' m', 0.0)\n",
      "\n",
      "Suggestion n°16\t,\t('ât', 1.0),\t('âte', 0.0),\t('ate', 0.0),\t('<|endoftext|>', 0.0),\t('at', 0.0)\n",
      "\n",
      "Suggestion n°17\t,\t('ée', 0.937),\t('é', 0.063),\t('<|endoftext|>', 0.0),\t('ées', 0.0),\t('e', 0.0)\n",
      "\n",
      "Suggestion n°18\t,\t(' ou', 0.886),\t(',', 0.113),\t(' pour', 0.001),\t(' et', 0.0),\t(' spéc', 0.0)\n",
      "\n",
      "Suggestion n°19\t,\t(' des', 0.404),\t(' encore', 0.237),\t(' de', 0.151),\t(' du', 0.138),\t(' par', 0.043)\n",
      "\n",
      "Suggestion n°20\t,\t(' rest', 0.459),\t(' fri', 0.266),\t(' mor', 0.222),\t(' petits', 0.022),\t(' al', 0.016)\n",
      "\n",
      "Suggestion n°21\t,\t('es', 1.0),\t('e', 0.0),\t('ants', 0.0),\t('<|endoftext|>', 0.0),\t('ent', 0.0)\n",
      "\n",
      "Suggestion n°22\t,\t(' de', 0.979),\t(' aliment', 0.017),\t('.', 0.003),\t(' d', 0.001),\t(' hum', 0.0)\n",
      "\n",
      "Suggestion n°23\t,\t(' vi', 0.539),\t(' rep', 0.237),\t(' table', 0.209),\t(' nour', 0.01),\t(' po', 0.003)\n",
      "\n",
      "Suggestion n°24\t,\t('ande', 0.999),\t('andes', 0.001),\t('a', 0.0),\t('an', 0.0),\t('<|endoftext|>', 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Les autres suggestions\n",
    "top_five = response['choices'][0][\"logprobs\"][\"top_logprobs\"]\n",
    "for k in range(len(top_five)) :\n",
    "  print('Suggestion n°'+str(k)+\"\\t\", *[(i[0], round(np.exp(i[1]),3)) for i in top_five[k].items()], sep=\",\\t\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rrIY-beq8X6"
   },
   "source": [
    "## Asking for a prompt, and output the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akKw7zu2oHgp",
    "outputId": "72c4310c-aeba-4688-8bf2-437c63447e86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt ?\n",
      "\n",
      "The cat is eating ...\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt ?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Ou \"davinci-002\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mx,  \u001b[38;5;66;03m# Votre texte d'entrée\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# Nombre maximal de tokens pour la réponse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Demander les logprobs pour les 5 meilleurs tokens\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      9\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    702\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "x = str(input(\"prompt ?\\n\\n\"))\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",  # Ou \"davinci-002\"\n",
    "    prompt=x,  # Votre texte d'entrée\n",
    "    max_tokens=50,  # Nombre maximal de tokens pour la réponse\n",
    "    logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "    top_p=1,\n",
    "    presence_penalty=0.5,\n",
    "    frequency_penalty=0.5\n",
    ")\n",
    "print(response['choices'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz7ya3dikN3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swGFt6hqrCsn"
   },
   "source": [
    "## Stock objects : tokens and probabilities\n",
    "Here we create objects, tokens is a list of the output tokens, logprobs is the file of their corresponding log probabilities, probs are the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_k-W_OUoHaX",
    "outputId": "656776be-cb55-459c-a907-fe5224fa31f9"
   },
   "outputs": [],
   "source": [
    "tokens = response['choices'][0][\"logprobs\"][\"tokens\"]\n",
    "\n",
    "logprobs = response['choices'][0][\"logprobs\"][\"token_logprobs\"]\n",
    "\n",
    "probs = [np.exp(i) for i in logprobs]\n",
    "\n",
    "print(*[(j, round(probs[i],2)) for i, j in enumerate(tokens)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgRDDGrIrcaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2Znn5iGrcV_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSRNkIDPrc9n"
   },
   "source": [
    "### We do the same with the top other candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LBy7p8KtvbV",
    "outputId": "2e5274b9-947f-4ff1-c394-2807d1e70d41"
   },
   "outputs": [],
   "source": [
    "# Les autres suggestions\n",
    "top_five = response['choices'][0][\"logprobs\"][\"top_logprobs\"]\n",
    "for k in range(len(top_five)) :\n",
    "  print('Suggestion n°'+str(k)+\"\\t\", *[(i[0], round(np.exp(i[1]),3)) for i in top_five[k].items()], sep=\",  \", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftImj0IXWKKY",
    "outputId": "a02d980b-d54c-4cb2-d08e-8ba78c9b378e"
   },
   "outputs": [],
   "source": [
    "x = str(input(\"prompt ?\\n\\n\"))\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",  # Ou \"davinci-002\"\n",
    "    prompt=x,  # Votre texte d'entrée\n",
    "    max_tokens=50,  # Nombre maximal de tokens pour la réponse\n",
    "    logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "    top_p=1,\n",
    "    presence_penalty=1,\n",
    "    frequency_penalty=2\n",
    ")\n",
    "print(response['choices'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99dHs96NWKT2",
    "outputId": "b748a289-3eb2-4b32-d794-122720c789bf"
   },
   "outputs": [],
   "source": [
    "probs = [np.exp(i) for i in response['choices'][0][\"logprobs\"][\"token_logprobs\"]]\n",
    "\n",
    "print(*[(j, round(probs[i],2)) for i, j in enumerate(response['choices'][0][\"logprobs\"][\"tokens\"])], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfOEnWrV5mpC"
   },
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K02dWBgEnZjS",
    "outputId": "41f14a30-ba97-43f3-878f-9edaa1204533"
   },
   "outputs": [],
   "source": [
    "x = str(input(\"prompt ?\\n\"))\n",
    "t = float(input(\"temperature ?\\n\"))\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",  # Ou \"davinci-002\"\n",
    "    prompt=x,  # Votre texte d'entrée\n",
    "    max_tokens=25,  # Nombre maximal de tokens pour la réponse\n",
    "    logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "    top_p=1,\n",
    "    presence_penalty=0,\n",
    "    frequency_penalty=0,\n",
    "    temperature=t\n",
    ")\n",
    "print(response['choices'][0][\"text\"])\n",
    "probs = [np.exp(i) for i in response['choices'][0][\"logprobs\"][\"token_logprobs\"]]\n",
    "print(*[(j, round(probs[i],2)) for i, j in enumerate(response['choices'][0][\"logprobs\"][\"tokens\"])], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo0GVrbdrnUQ"
   },
   "source": [
    "# Presence penalty, frequency penalty, temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1OknzgMuX7S"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "  x = str(input(\"prompt ?\\n\\n\"))\n",
    "  p = float(input(\"penalty ?\\n\\n\"))\n",
    "  f = float(input(\"frequency ?\\n\\n\"))\n",
    "  t = float(input(\"temperature ?\\n\\n\"))\n",
    "  response = openai.Completion.create(\n",
    "      model=\"gpt-3.5-turbo-instruct\",  # Ou \"davinci-002\"\n",
    "      prompt=x,  # Votre texte d'entrée\n",
    "      max_tokens=25,  # Nombre maximal de tokens pour la réponse\n",
    "      logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "      top_p=1,\n",
    "      presence_penalty=p,\n",
    "      frequency_penalty=f,\n",
    "      temperature=t\n",
    "  )\n",
    "  print(response['choices'][0][\"text\"], end=\"\\n\")\n",
    "  #probs = [np.exp(i) for i in response['choices'][0][\"logprobs\"][\"token_logprobs\"]]\n",
    "  #print(*[(j, round(probs[i],2)) for i, j in enumerate(response['choices'][0][\"logprobs\"][\"tokens\"])], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSAk8WOjuX5B",
    "outputId": "6e256245-5963-4a00-db3f-49fa08a4e000"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6fu-eSF7YNB",
    "outputId": "6e504bd4-6c77-44bd-e218-f913a7919948"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6vcZMWj7YKN",
    "outputId": "2fdc1188-fd3e-4cee-881b-e9a2b965df2a"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-uIoP_-7YG1",
    "outputId": "e27a2125-7875-4b08-f31b-aa76bee2b21b"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4R7vdo3t7YDs",
    "outputId": "8e94cca6-0c83-46b1-ebe5-749102b7219b"
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrGlVgCl7YA_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al4vpghJuX18"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Le_y8wFYuXzL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cENDE-H_8GxD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaOYWeTc8Gn0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJE7qVFu8Gkw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TjmRKch8Gh4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgS3n6B_8Gfc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qed9caDj8Gcc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwYNqJZk8GZf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO_i479M8GWf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1JYeXL_8GTS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CftC2AYr8GQ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ibXFksN8GOA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbDIGgTZ8GKL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sNwOlPP8GHI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F-_rcdw8GDv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7ZsSI1g8GBC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9_VopM-8F97"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAJVMuCu8F6r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sogSe4Cu8F3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSFXGYXG8F0i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA6PY5ks8Fxo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR6hQvNp8Fuq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REt_BX1suXwA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ-W6sFguXtW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es57sEnwuXpX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyxNnIQDuch3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOgoev1vuc6Y"
   },
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "<h1>Draft</h1>\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWs0OhXBuXly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7uGzU0LXw1R",
    "outputId": "d9514252-6c5b-4f0d-f57a-64d1c7c90f51"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Effectuer une requête avec l'API de complétion pour obtenir les logprobs\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",  # Ou \"gpt-4\" si vous y avez accès\n",
    "    prompt=\"Le chat mange...\",  # Votre texte d'entrée\n",
    "    max_tokens=50,  # Nombre maximal de tokens pour la réponse\n",
    "    logprobs=5,  # Demander les logprobs pour les 5 meilleurs tokens\n",
    "    top_p=1,\n",
    "    presence_penalty=0.5,\n",
    "    frequency_penalty=0.5\n",
    ")\n",
    "\n",
    "def transformer_logprobs_en_liste(choice_logprobs):\n",
    "    \"\"\"\n",
    "    Transforme les logprobs en une structure liste de listes.\n",
    "\n",
    "    Args:\n",
    "        choice_logprobs (list): Liste des logprobs par token.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire où chaque token est une clé et la valeur est une liste de tuples (token, probabilité).\n",
    "    \"\"\"\n",
    "    resultat = {}\n",
    "    for idx, token_logprob in enumerate(choice_logprobs):\n",
    "        token_cle = f\"token_{idx + 1}\"\n",
    "        candidats = []\n",
    "        for token, logprob in token_logprob.items():\n",
    "            proba = math.exp(logprob)  # Convertir logprob en probabilité\n",
    "            candidats.append((token, proba))\n",
    "        resultat[token_cle] = candidats\n",
    "    return resultat\n",
    "\n",
    "# Afficher la réponse générée et les logprobs transformés\n",
    "for choice in response['choices']:\n",
    "    print(f\"Texte généré : {choice['text']}\")\n",
    "    if 'logprobs' in choice:\n",
    "        # Transformer les logprobs\n",
    "        logprobs_top = choice['logprobs']['top_logprobs']\n",
    "        resultat_structure = transformer_logprobs_en_liste(logprobs_top)\n",
    "\n",
    "        # Affichage en JSON pour lisibilité\n",
    "        print(\"Logprobs structurés :\")\n",
    "        print(json.dumps(resultat_structure, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ve8BuxxabGM5",
    "outputId": "11b18106-7d0f-41ab-cb03-31db114db5e4"
   },
   "outputs": [],
   "source": [
    "response['choices'][0][\"logprobs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jodi236Gnrua"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-KNYIsfZ41r",
    "outputId": "bbc9cfeb-4c3d-4596-d1ee-c6840c2ce499"
   },
   "outputs": [],
   "source": [
    "print([i for i in response['choices'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ek7gYdT1ahB5",
    "outputId": "51bba994-f29e-4ae6-bc14-3a3783734083"
   },
   "outputs": [],
   "source": [
    "print(response['choices'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25ynrqMMbMvi",
    "outputId": "85cfedb7-b0c9-4c1c-cb6e-5145f78fdd36"
   },
   "outputs": [],
   "source": [
    "print([i for i in response['choices'][0][\"logprobs\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcAyQta5bU8C",
    "outputId": "0b22fe16-a0f1-4cf7-841e-ee1cbc9f8786"
   },
   "outputs": [],
   "source": [
    "response['choices'][0][\"logprobs\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SXD6ZTWbiPX",
    "outputId": "a9e0f42c-a8f1-4c0e-edd9-296e49940b95"
   },
   "outputs": [],
   "source": [
    "[np.exp(i) for i in response['choices'][0][\"logprobs\"][\"token_logprobs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQUD6fmxbuI9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
