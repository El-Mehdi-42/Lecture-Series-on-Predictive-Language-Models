{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2- Accessing Text Corpora</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gutenberg Corpus</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "emma = gutenberg.words('austen-emma.txt')\n",
    "print(emma[:20])\n",
    "print(len(emma), len(set(emma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emma)/len(set(emma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in gutenberg.sents(\"shakespeare-macbeth.txt\")[10:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.Text(emma)\n",
    "emma.concordance(\"surprize\", lines=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Mr. Obama has explained that. Fifa world cup (year 2022)\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nltk.word_tokenize(s) for s in nltk.sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><i>Join function:</i></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def join_funct(sentence):\n",
    "    sentence = ' '.join(sentence) # join normally\n",
    "    return(sentence)\n",
    "\n",
    "longest_se = max([len(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")])\n",
    "longest_sent = [s for s in gutenberg.sents(\"shakespeare-macbeth.txt\") if len(s) == longest_se]\n",
    "print(*longest_sent[0], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(\" ([,.;\\):])\", lambda m: m.group(1), \"Mr. seguin ( propriétaire de la chèvre ) nous parle de lui ( Merci à lui ) .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def join_func(sentence):\n",
    "    sentence = ' '.join(sentence) # join normally\n",
    "    sentence = re.sub(\" ([,.;\\):])\", lambda m: m.group(1), sentence) # stick to left\n",
    "    sentence = re.sub(\"([\\(]) \", lambda m: m.group(1), sentence) # stick to right\n",
    "    sentence = re.sub(\" ([']) \", lambda m: m.group(1), sentence) # join both sides\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "longest_se = max([len(s) for s in macbeth_sentences])\n",
    "longest_sen = [s for s in macbeth_sentences if len(s) == longest_se]\n",
    "longest_sent = ' '.join(longest_sen[0])\n",
    "print(\"First method \\n\", longest_sent, \"\\n Second method \\n\", join_func(longest_sen[0]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
    "    print(int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Web and Chat Text</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Although Project Gutenberg contains thousands of books, it represents established\n",
    "literature. It is important to consider less formal language as well. NLTK’s small collection\n",
    "of web text includes content from a Firefox discussion forum, conversations\n",
    "overheard in New York, the movie script of <i>Pirates of the Carribean</i>, personal advertisements, and wine reviews:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, \":\", webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(webtext.raw(fileid)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "print(join_func(chatroom[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"red\">Brown Corpus</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Brown Corpus was the first million-word electronic corpus of English, created in\n",
    "1961 at Brown University. This corpus contains text from 500 sources, and the sources\n",
    "have been categorized by genre, such as news, editorial, and so on. Table 2-1 gives an\n",
    "    example of each genre (for a complete list, see <a>http://icame.uib.no/brown/bcm-los.html</a>).</p>\n",
    "\n",
    "<p>We can access the corpus as a list of words or a list of sentences (where each sentence\n",
    "is itself just a list of words). We can optionally specify particular categories or files to\n",
    "read:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(categories='adventure')[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(fileids=['cg22'])[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter, value in enumerate(brown.sents(categories=['news', 'editorial', 'reviews'])[1:7]):\n",
    "    print(str(counter), \"- \", join_func(value), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m],) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reuters Corpus</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The\n",
    "documents have been classified into 90 topics, and grouped into two sets, called “training”\n",
    "and “test”; thus, the text with fileid <FONT face=\"courier\", size=\"3\">'test/14826'</FONT> is a document drawn from the\n",
    "test set. This split is for training and testing algorithms that automatically detect the\n",
    "topic of a document, as we will see in Chapter 6.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "#reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Inaugural Address Corpus</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fileid[:4] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['man', 'women',\"people\"]\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Annotated Text Corpora</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Corpora in Other Languages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_func(nltk.corpus.cess_esp.words()[0:100])  # Espangol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.floresta.words()  # Italien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.indian.words('hindi.pos')  # Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (lang, len(word))\n",
    "    for lang in languages\n",
    "    for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udhr.fileids()\n",
    "raw_text = udhr.raw('Arabic_Alarabia-Arabic')\n",
    "nltk.FreqDist(raw_text).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "print(raw[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
    "print(words[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
    "print(sents[1:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(treebank.words())\n",
    "tree1 = treebank.parsed_sents()[0:1]\n",
    "print(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Counting Words by Genre</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting and Tabulating Distributions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "cfd1 = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target))\n",
    "cfd1.plot(cumulative=True, title=\"Freq of words ('amercia' & 'citizen') from 1789 until 2009 in inaugural corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd2.plot(conditions=[\"English\",\"French_Francais\"],samples=range(25), cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>More Bigrams</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
    "sent2 = []\n",
    "for i in nltk.bigrams(sent):\n",
    "    sent2.append(list(i))\n",
    "    \n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "text_bigram = []\n",
    "for s in inaugural.sents():\n",
    "    for w in nltk.bigrams(s):\n",
    "        text_bigram.append(list(w))\n",
    "    \n",
    "print(text_bigram[:80])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Names Corpus</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "print([w for w in male_names if w in female_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (fileid, name[-1])\n",
    "    for fileid in names.fileids()\n",
    "    for name in names.words(fileid))\n",
    "cfd.plot(title=\"Conditional frequency distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A Pronouncing Dictionary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = nltk.corpus.cmudict.entries()\n",
    "print(len(entries), \"\\n\")\n",
    "for entry in entries[39943:39951]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For each word, this lexicon provides a list of phonetic codes—distinct labels for each contrastive sound—known as phones. Observe that fire has two pronunciations (in U.S. English): the one-syllable <font face=\"courier\",size=\"3\"> F AY1 R</font>, and the two-syllable <font face=\"courier\",size=\"3\">F AY1 ER0</font>. The symbols in the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at <a>http://en.wikipedia.org/wiki/Arpabet.</a></p>\n",
    "<p>Each entry consists of two parts, and we can process these individually using a more complex version of the <font face=\"courier\", size=\"3\">for</font> statement. Instead of writing <font face=\"courier\", size=\"3\">for entry in entries:</font>, we replace <font face=\"courier\", size=\"3\">entry</font> with two variable names, <font face=\"courier\", size=\"3\">word</font>, <font face=\"courier\", size=\"3\">pron</font> . Now, each time through the loop, <font face=\"courier\", size=\"3\">word</font> is assigned the first part of the entry, and <font face=\"courier\", size=\"3\">pron</font> is assigned the second part of the entry:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        ph_1, ph_2, ph_3 = pron\n",
    "        if ph_1 == 'P' and ph_3 == 'T':\n",
    "            print(word, ph_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable = ['N', 'IH0', 'K', 'S']\n",
    "print([word for word, pron in entries if pron[-4:] == syllable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress(pron):\n",
    "    return [char for phone in pron for char in phone if char.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = [(pron[0]+'-'+pron[2], word)\n",
    "    for (word, pron) in entries\n",
    "    if pron[0] == 'P' and len(pron) == 3]\n",
    "cfd = nltk.ConditionalFreqDist(p3)\n",
    "for template in cfd.conditions():\n",
    "    if len(cfd[template]) > 10:\n",
    "        words = cfd[template].keys()\n",
    "        wordlist = ' '.join(words)\n",
    "        print(template, wordlist[:70] + \" ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()\n",
    "prondict['fire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using datasets package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "# Get a list of all available dataset names\n",
    "all_datasets = list_datasets()\n",
    "print(len(all_datasets), all_datasets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, only ```en``` and ```en-basic``` are supported by the ```nltk.corpus.words``` interface.\n",
    "\n",
    "There are other things like ```lowercase```, ```lemmatize/stemming``` etc. other than ```word_tokenize``` to consider but hopefully the data source above will be a good start to find words in the language you need beyond the English words that ```nltk.corpus.words``` provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case\n",
    "# pip install -U apache_beam\n",
    "# pip install -U datasets\n",
    "# pip install -U nltk\n",
    "# pip install -U tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tqdm``` like the word in arabic taqadum that means progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import nltk\n",
    "#nltk.download('popular')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.de\")\n",
    "\n",
    "\n",
    "num_texts = len(dataset['train']) # outputs: 2,665,357\n",
    "\n",
    "\n",
    "# Pick the first 100,000 texts\n",
    "de_words = set(chain(*(word_tokenize(dataset['train'][i]['text']) for i in tqdm(range(100_000)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(de_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*list(de_words)[:42], sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_words_part = list(de_words)[:42]\n",
    "positions = list(map(lambda x: 0 if any(i.isdigit() for i in x) else 1, de_words_part))\n",
    "print(*positions[:15], sep=', ', end='\\n\\n')\n",
    "\n",
    "popped_out = []\n",
    "k = 0\n",
    "\n",
    "for i, j in enumerate(positions):\n",
    "    if not j :\n",
    "        i -= k\n",
    "        popped_out.append(de_words_part.pop(i))\n",
    "        k += 1\n",
    "\n",
    "print(*de_words_part, sep=', ', end='\\n\\n')\n",
    "print(*popped_out, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other known Natural Language Processing (NLP) Datasets:\n",
    "- **GLUE**: A collection of nine different NLP tasks for evaluating language understanding.\n",
    "- **SQuAD**: The Stanford Question Answering Dataset, widely used for reading comprehension and QA.\n",
    "- **CNN/DailyMail**: A dataset for summarization tasks based on news articles.\n",
    "- **CoNLL-2003**: For named entity recognition (NER).\n",
    "- **Wikitext**: A large-scale language modeling dataset extracted from Wikipedia.\n",
    "- **IMDB**: A sentiment analysis dataset composed of movie reviews.\n",
    "\n",
    "See <a>https://huggingface.co/</a> for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to extract the *'hamim-87/Ashrafur_bangla_math'* dataset \n",
    "\n",
    "```small_test = load_dataset('hamim-87/Ashrafur_bangla_math')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
